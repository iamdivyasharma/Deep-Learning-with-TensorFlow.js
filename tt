# orchestrator_flask_mcp_agentic.py
from __future__ import annotations
import json, time, concurrent.futures, hashlib
from typing import Optional, Dict, Any, List, Tuple
from collections import defaultdict, deque
from time import time as now

import requests
import boto3
from botocore.config import Config
from botocore.exceptions import BotoCoreError, ClientError
from flask import Flask, request, jsonify

# ========= Hard-coded backends =========
SERVICES = {
    "air":   {"url": "http://localhost:9003/air_query",   "field": "qquery"},
    "hotel": {"url": "http://localhost:9001/hotel_query", "field": "qquery"},
    "tdna":  {"url": "http://localhost:9002/tdna_query",  "field": "qquery"},
}
BASE_URL = "http://localhost:9000"   # for OpenAPI only

# Requests timeouts: (connect, read)
REQ_TIMEOUT: Tuple[int,int] = (5, 60)

# ========= Bedrock (Claude) =========
BEDROCK_REGION    = "us-east-1"
CLAUDE_MODEL_ID   = "anthropic.claude-3-5-sonnet-20240620-v1:0"
ANTHROPIC_VERSION = "bedrock-2023-05-31"
bedrock = boto3.client(
    "bedrock-runtime",
    region_name=BEDROCK_REGION,
    config=Config(connect_timeout=5, read_timeout=30, retries={"max_attempts": 2, "mode":"standard"})
)

# ========= Memory (short-term, per session) =========
MEMORY: Dict[str, Dict[str, Any]] = defaultdict(lambda: {"history": deque(maxlen=20), "created": now()})
def mem_append(session_id: str, role: str, content: Any):
    MEMORY[session_id]["history"].append({"role": role, "content": content, "ts": now()})
def mem_get(session_id: str, k: int = 10) -> List[Dict[str, Any]]:
    hist = list(MEMORY[session_id]["history"]); return hist[-k:] if k else hist
def mem_clear(session_id: str): MEMORY.pop(session_id, None)

# ========= Simple TTL Cache (in-process) =========
CACHE_TTL_SEC = 300  # 5 minutes
_CACHE: Dict[str, Tuple[float, Any]] = {}

def _norm_extra(extra: Optional[Dict[str, Any]]) -> str:
    try: return json.dumps(extra or {}, sort_keys=True, separators=(",",":"))
    except Exception: return "{}"

def _hash(obj: Any) -> str:
    s = obj if isinstance(obj, str) else json.dumps(obj, sort_keys=True, default=str)
    return hashlib.sha256(s.encode("utf-8")).hexdigest()[:16]

def cache_get(key: str):
    ent = _CACHE.get(key)
    if not ent: return None
    exp, val = ent
    if exp < now():
        _CACHE.pop(key, None)
        return None
    return val

def cache_set(key: str, val: Any, ttl: int = CACHE_TTL_SEC):
    _CACHE[key] = (now() + ttl, val)

# ========= Heuristic fallback router =========
AIR_WORDS   = {"air","flight","flights","airline","pnr","ticket","fare","baggage","seat","airport"}
HOTEL_WORDS = {"hotel","room","rooms","stay","booking","reservation","rate","price","amenities","check-in","check out"}
TDNA_WORDS  = {"tdna","dna","genome","variant","gene","sequence","omics"}
def _score(t: str, vocab: set[str]) -> int: return sum(w in t.lower() for w in vocab)
def _fallback_targets(q: str) -> List[str]:
    a, h, t = _score(q, AIR_WORDS), _score(q, HOTEL_WORDS), _score(q, TDNA_WORDS)
    picks: List[str] = []
    if a and h: picks.extend(["air","hotel"])
    if a and t: picks.extend(["air","tdna"])
    if h and t: picks.extend(["hotel","tdna"])
    if picks:
        seen, out = set(), []
        for k in picks:
            if k not in seen: out.append(k); seen.add(k)
        return out
    best = max([("air", a), ("hotel", h), ("tdna", t)], key=lambda kv: kv[1])
    return [best[0] if best[1] > 0 else "hotel"]

# ========= Claude helpers =========
def _invoke_bedrock(model_id: str, body: dict) -> dict:
    resp = bedrock.invoke_model(modelId=model_id, body=json.dumps(body))
    return json.loads(resp["body"].read().decode("utf-8"))

def claude_json(model_id: str, system: str, user_obj: dict, max_tokens=512, temperature=0):
    body = {
        "anthropic_version": ANTHROPIC_VERSION,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "system": system,
        "messages": [{"role":"user","content":[{"type":"text","text": json.dumps(user_obj)}]}],
    }
    payload = _invoke_bedrock(model_id, body)
    text = payload["content"][0]["text"]
    return json.loads(text)

def claude_text(model_id: str, system: str, user_obj: dict, max_tokens=512, temperature=0):
    body = {
        "anthropic_version": ANTHROPIC_VERSION,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "system": system,
        "messages": [{"role":"user","content":[{"type":"text","text": json.dumps(user_obj)}]}],
    }
    payload = _invoke_bedrock(model_id, body)
    return payload["content"][0]["text"]

# ========= Router prompt =========
ROUTER_SYSTEM = (
    "You are a strict router for a multi-tool system.\n"
    "Tools: air, hotel, tdna. Pick one or more tools that should handle the user's query.\n"
    'Return ONLY JSON like: {"targets": ["air" | "hotel" | "tdna", ...]}'
)
def llm_decide_targets(query: str) -> List[str]:
    try:
        payload = _invoke_bedrock(CLAUDE_MODEL_ID, {
            "anthropic_version": ANTHROPIC_VERSION,
            "max_tokens": 128,
            "temperature": 0,
            "system": ROUTER_SYSTEM,
            "messages": [{"role":"user","content":[{"type":"text","text": query}]}],
        })
        data = json.loads(payload["content"][0]["text"])
        targets = [t for t in data.get("targets", []) if t in SERVICES]
        return targets or _fallback_targets(query)
    except Exception:
        return _fallback_targets(query)

# ========= Backend call (with cache + tolerant timeouts) =========
def call_backend(target: str, query: str, extra: Optional[Dict[str, Any]]):
    svc = SERVICES[target]
    payload = {svc["field"]: query}
    if isinstance(extra, dict): payload.update(extra)

    # cache key per target+query+extra
    key = f"bot:{target}:{_hash(query)}:{_hash(_norm_extra(extra))}"
    hit = cache_get(key)
    if hit is not None:
        return hit  # cached response

    try:
        r = requests.post(svc["url"], json=payload, timeout=REQ_TIMEOUT)
        if r.status_code == 422:
            r = requests.post(svc["url"], json={"input": payload}, timeout=REQ_TIMEOUT)
        r.raise_for_status()
        data = r.json()
        value = data.get("answer", data)
        cache_set(key, value)
        return value
    except requests.exceptions.ReadTimeout:
        return f"{target} service timed out after {REQ_TIMEOUT[1]}s"
    except requests.exceptions.ConnectTimeout:
        return f"{target} service failed to connect within {REQ_TIMEOUT[0]}s"
    except Exception as e:
        return f"{type(e).__name__}: {e}"

# ========= Composer (with cache) =========
COMPOSER_SYSTEM = (
    "You are a response composer. Combine multiple tool answers (air/hotel/tdna) "
    "into one clear, concise reply for the user. Do not include tool names unless helpful. "
    "Keep it factual and actionable. If information conflicts, say so briefly and provide guidance."
)
def compose_final_answer(user_query: str, answers_by_target: Dict[str, Any]) -> str:
    if not answers_by_target: return ""
    # cache key for composition depends on query + targets + hashed answers
    answers_fingerprint = _hash({k: _hash(v) for k, v in sorted(answers_by_target.items())})
    ckey = f"compose:{_hash(user_query)}:{answers_fingerprint}"
    chit = cache_get(ckey)
    if chit is not None:
        return chit
    try:
        text = claude_text(
            model_id=CLAUDE_MODEL_ID,
            system=COMPOSER_SYSTEM,
            user_obj={"user_query": user_query, "answers_by_target": answers_by_target},
            max_tokens=512, temperature=0
        ).strip()
        cache_set(ckey, text)
        return text
    except Exception:
        # simple deterministic fallback
        parts = []
        if "hotel" in answers_by_target: parts.append(f"Hotel: {answers_by_target['hotel']}")
        if "air"   in answers_by_target: parts.append(f"Air: {answers_by_target['air']}")
        if "tdna"  in answers_by_target: parts.append(f"TDNA: {answers_by_target['tdna']}")
        text = " | ".join(str(p) for p in parts if p)
        cache_set(ckey, text)
        return text

# ========= Agent prompts =========
PLANNER_SYSTEM = """You are an orchestration planner.
Tools: air, hotel, tdna.
Given the user query, prior memory, and latest observations, return ONLY JSON:
{"steps":[{"target":"air"|"hotel"|"tdna","input":"<text for qquery>"}...],"finish":false,"final_answer":""}
Prefer parallel steps when independent; otherwise sequence. Use memory if relevant; JSON only."""
FINISHER_SYSTEM = 'If observations+memory satisfy the goal, return {"finish": true, "final_answer": "..."} else {"finish": false}.'

# ========= Flask app =========
app = Flask(__name__)

# Memory helpers (optional endpoints)
@app.post("/memory/clear")
def memory_clear():
    sid = (request.get_json(force=True) or {}).get("session_id", "default"); mem_clear(sid)
    return jsonify({"ok": True, "session_id": sid})
@app.get("/memory/show")
def memory_show():
    sid = request.args.get("session_id", "default"); return jsonify({"session_id": sid, "history": mem_get(sid, k=0)})

# ---------- One-shot router (parallel fan-out) ----------
@app.post("/route_query")
def route_query():
    body   = request.get_json(force=True) or {}
    query  = body.get("query", "")
    extra  = body.get("extra")
    explicit_targets = body.get("targets")
    forced = body.get("target")

    if explicit_targets and isinstance(explicit_targets, list):
        targets = [t for t in explicit_targets if t in SERVICES]; routed_by = "hint:list"
    elif forced == "both":
        targets, routed_by = ["hotel","tdna"], "hint:both"
    elif forced in SERVICES:
        targets, routed_by = [forced], "hint:single"
    else:
        targets, routed_by = llm_decide_targets(query), "llm"

    results, errors = {}, {}
    if len(targets) <= 1:
        tgt = targets[0] if targets else "hotel"
        res = call_backend(tgt, query, extra)
        results[tgt] = res
    else:
        # PARALLEL fan-out to both bots (simultaneous)
        with concurrent.futures.ThreadPoolExecutor(max_workers=len(targets)) as ex:
            futs = {ex.submit(call_backend, tgt, query, extra): tgt for tgt in targets}
            for fut in concurrent.futures.as_completed(futs):
                tgt = futs[fut]
                try: results[tgt] = fut.result()
                except Exception as e: errors[tgt] = f"{type(e).__name__}: {e}"

    # Compose one user-ready message when multiple
    if len(results) > 1:
        merged = compose_final_answer(query, results)
    else:
        merged = next(iter(results.values()), "")

    status = 200 if results else 502
    return jsonify({
        "tool": "route_query_tool",
        "targets": targets,
        "routed_by": routed_by,
        "answer": merged,
        "answers_by_target": results if len(results) > 1 else None,
        "errors": errors or None
    }), status

# ---------- Agentic multi-step ----------
@app.post("/agent_query")
def agent_query():
    body        = request.get_json(force=True) or {}
    user_query  = body.get("query", "")
    extra       = body.get("extra")
    session_id  = body.get("session_id", "default")
    max_iters   = int(body.get("max_iters", 3))
    parallel_cap= int(body.get("parallel_cap", 3))
    autonomy    = bool(body.get("autonomy", True))
    remember    = bool(body.get("remember", True))

    mem_append(session_id, "user", user_query)
    observations: List[Dict[str, Any]] = []
    final_answer = None

    def execute_steps(steps: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        steps = [s for s in steps if s.get("target") in SERVICES][:parallel_cap]
        if not steps: return []
        out: List[Dict[str, Any]] = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=len(steps)) as ex:
            futs = {ex.submit(call_backend, s["target"], s.get("input", user_query), extra): s for s in steps}
            for fut in concurrent.futures.as_completed(futs):
                step = futs[fut]
                try: out.append({"step": step, "result": fut.result()})
                except Exception as e: out.append({"step": step, "error": f"{type(e).__name__}: {e}"})
        return out

    # Planner/Finisher helpers (with small caches)
    def plan_once(obs_slice):
        pkey = f"plan:{_hash(user_query)}:{_hash(obs_slice)}:{_hash(mem_get(session_id, k=4))}"
        hit = cache_get(pkey)
        if hit is not None: return hit
        try:
            plan = claude_json(CLAUDE_MODEL_ID, PLANNER_SYSTEM,
                               {"user_query": user_query, "memory": mem_get(session_id, k=8), "observations": obs_slice},
                               max_tokens=512, temperature=0)
        except Exception:
            targets = llm_decide_targets(user_query)
            plan = {"steps":[{"target": targets[0] if targets else "hotel", "input": user_query}],
                    "finish": False, "final_answer": ""}
        cache_set(pkey, plan, ttl=90)
        return plan

    def finish_once(batch):
        fkey = f"finish:{_hash(user_query)}:{_hash(batch)}:{_hash(mem_get(session_id, k=4))}"
        hit = cache_get(fkey)
        if hit is not None: return hit
        try:
            fin = claude_json(CLAUDE_MODEL_ID, FINISHER_SYSTEM,
                              {"user_query": user_query, "memory": mem_get(session_id, k=8), "observations": batch},
                              max_tokens=256, temperature=0)
        except Exception:
            fin = {"finish": False}
        cache_set(fkey, fin, ttl=90)
        return fin

    for _ in range(max_iters):
        obs_slice = observations[-3:]
        plan = plan_once(obs_slice)
        if plan.get("finish") and plan.get("final_answer"):
            final_answer = plan["final_answer"]; break

        steps = plan.get("steps", [])
        if not steps: break

        batch = execute_steps(steps)
        observations.extend(batch)

        if not autonomy:
            latest_per_target: Dict[str, Any] = {}
            for ob in reversed(batch):
                tgt = ob.get("step",{}).get("target")
                if tgt and "result" in ob and tgt not in latest_per_target:
                    latest_per_target[tgt] = ob["result"]
            final_answer = compose_final_answer(user_query, latest_per_target) if latest_per_target else "No result."
            break

        fin = finish_once(batch)
        if fin.get("finish"):
            final_answer = fin.get("final_answer", None)
            break

    if final_answer is None:
        latest_per_target: Dict[str, Any] = {}
        for ob in reversed(observations):
            tgt = ob.get("step",{}).get("target")
            if tgt and "result" in ob and tgt not in latest_per_target:
                latest_per_target[tgt] = ob["result"]
        final_answer = compose_final_answer(user_query, latest_per_target) if latest_per_target else "No result."

    if remember: mem_append(session_id, "assistant", final_answer)

    return jsonify({
        "tool": "agent_query_tool",
        "session_id": session_id,
        "iterations": min(len(observations), max_iters),
        "final_answer": final_answer,
        "observations": observations[-6:],
        "autonomy": autonomy
    }), 200

# ---------- Health & OpenAPI ----------
@app.get("/healthz")
def healthz(): return jsonify({"ok": True})

@app.get("/openapi.json")
def openapi():
    return jsonify({
        "openapi": "3.0.3",
        "info": {"title": "Orchestrator MCP (Flask + Agentic + Cache)", "version": "0.6.0"},
        "servers": [{"url": BASE_URL}],
        "paths": {
            "/route_query": {"post": {
                "operationId": "route_query_tool",
                "summary": "One-shot routing (parallel) to air/hotel/tdna; cached",
                "requestBody": {"required": True, "content": {"application/json": {"schema": {"$ref": "#/components/schemas/RouteInput"}}}},
                "responses": {"200": {"description": "OK", "content": {"application/json": {"schema": {"$ref": "#/components/schemas/RouteOutput"}}}}}
            }},
            "/agent_query": {"post": {
                "operationId": "agent_query_tool",
                "summary": "Agentic plan→act→reflect with memory; cached planning/finishing",
                "requestBody": {"required": True, "content": {"application/json": {"schema": {"$ref": "#/components/schemas/AgentInput"}}}},
                "responses": {"200": {"description": "OK", "content": {"application/json": {"schema": {"$ref": "#/components/schemas/AgentOutput"}}}}}
            }}
        },
        "components": {"schemas": {
            "RouteInput": {"type":"object","properties":{
                "query":{"type":"string"},
                "extra":{"type":"object","additionalProperties":True},
                "target":{"type":"string","enum":["air","hotel","tdna","both"]},
                "targets":{"type":"array","items":{"type":"string","enum":["air","hotel","tdna"]}}
            },"required":["query"]},
            "RouteOutput":{"type":"object","properties":{
                "tool":{"type":"string"},
                "targets":{"type":"array","items":{"type":"string"}},
                "routed_by":{"type":"string"},
                "answer":{},
                "answers_by_target":{"type":"object","additionalProperties":True},
                "errors":{"type":"object","additionalProperties":{"type":"string"}}
            },"required":["tool","targets","answer"]},
            "AgentInput":{"type":"object","properties":{
                "query":{"type":"string"},
                "extra":{"type":"object","additionalProperties":True},
                "session_id":{"type":"string"},
                "max_iters":{"type":"integer","minimum":1,"maximum":8},
                "parallel_cap":{"type":"integer","minimum":1,"maximum":5},
                "autonomy":{"type":"boolean"},
                "remember":{"type":"boolean"}
            },"required":["query"]},
            "AgentOutput":{"type":"object","properties":{
                "tool":{"type":"string"},
                "session_id":{"type":"string"},
                "iterations":{"type":"integer"},
                "final_answer":{},
                "observations":{"type":"array","items":{"type":"object"}},
                "autonomy":{"type":"boolean"}
            },"required":["tool","final_answer"]}
        }}
    })

if __name__ == "__main__":
    # pip install flask requests boto3 botocore
    app.run(host="0.0.0.0", port=9000)
